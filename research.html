<!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" href="./style.css" type="text/css" />
    <title>Research</title>
</head>
<body>
    <h1 style="padding-left: 0.5em">Jiangchao Yao</h1>
    <hr>
    <table id="tlayout">
        <td id="layout-menu">
            <div class="menu-item"><a href="index.html" class="current">Home</a></div>
            <div class="menu-item"><a href="research.html" class="current">Research</a></div>
            <div class="menu-item"><a href="publication.html" class="current">Publication</a></div>
            <div class="menu-item"><a href="service.html" class="current">Professional Service</a></div>
            <div class="menu-item"><a href="teaching.html" class="current">Teaching</a></div>
            <div class="menu-item"><a href="JoinUs.html" class="current">Join Us</a></div>
        </td>

        <td id="layout-content">
            <h1 style="margin-top: 0em">Reseach Focus</h1><br>

            <div>
                <h1>
                    <hr>
                    Label-noise and Adversarially Robust Learning
                </h1>
                <p>
                    Perturbation can be ubiquitous in the real-world data and the proper mass can actually robustify the training of machine
                    learning algorithms. This is common in the training practice like dropout and data augmentation with randomness. However,
                    when it is excessive or deliberate, the special design should be considered in the training methods to reduce their negative
                    impact. Motivated by this belief, we developed a range of methods to provide the references on this way.
                </p>
                <ul>
                    <li>
                        <p>Label-noise learning under the class-conditional assumption [<a href="https://dl.acm.org/doi/pdf/10.5555/3327345.3327485">NeurIPS'18</a>, <a href="https://ojs.aaai.org/index.php/AAAI/article/view/4943">AAAI'19</a>, <a href="https://www.computer.org/csdl/journal/tp/2023/08/10049697/1KYog7RZSUg">TPAMI'23</a>]</p>
                    </li>
                    <li>
                        <p>Label-noise learning under the selection, correction and regularization [<a href="http://proceedings.mlr.press/v97/yu19b/yu19b.pdf">ICML'19</a>, <a href="https://ieeexplore.ieee.org/abstract/document/8506425">TIP'19</a>, <a href="https://ojs.aaai.org/index.php/AAAI/article/view/17222">AAAI'21</a>, <a href="">NeurIPS'23</a>] </p>
                    </li>
                    <li>
                        <p>Adversarial Learning with distillation, slack or representation learning. [<a href="https://openreview.net/forum?id=u6TRGdzhfip">ICLR'22</a>, <a href="https://openreview.net/forum?id=eKllxpLOOm">ICLR'23</a>], <a href="https://openreview.net/pdf?id=Vcl3qckVyh">ICML'23</a></p>
                    </li>
                </ul>
            </div>

            <div>
                <h1>
                    <hr>
                    Class- Subpopulation- and Domain- Imbalance Learning
                </h1>
                <p>

                </p>
            </div>

            <div>
                <h1>
                    <hr>
                    Multimodal Robust Representation Learning
                </h1>
                <p>
                    <h2>TBD</h2>
                </p>
            </div>

            <div>
                <h1>
                    <hr>
                    Universal Pretraining Methods for Medical Imaging Diagnosis
                </h1>
                <p>
                    <h2>TBD</h2>
                </p>
            </div>
        </td>

    </table>
</body>
</html>