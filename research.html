<!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" href="./style.css" type="text/css" />
    <title>Research</title>
</head>
<body>
    <h1 style="padding-left: 0.5em">Jiangchao Yao</h1>
    <hr>
    <table id="tlayout">
        <td id="layout-menu">
            <div class="menu-item"><a href="index.html">Home</a></div>
            <div class="menu-item"><a href="research.html" class="current">Research</a></div>
            <div class="menu-item"><a href="publication.html">Publication</a></div>
            <div class="menu-item"><a href="service.html">Service</a></div>
            <div class="menu-item"><a href="teaching.html">Teaching</a></div>
            <div class="menu-item"><a href="JoinUs.html">Join Us</a></div>
        </td>

        <td id="layout-content">
            <h1 style="margin-top: 0em">Reseach Focus</h1>
            [Topics change stage by stage along with the trending challenges in AI]
            <div>
                <h1>
                    <hr>
                    Foundation models and MedAI applications
                </h1>
                <ul><li>
                        <p> Reasoning [AR-bench (<a href="https://arxiv.org/pdf/2506.08295">ICML'25</a>)] </p>
                    </li>
                    <li>
                        <p> Acceleration [DivBS (<a href="https://openreview.net/pdf?id=5QWKec0eDF">ICML'24</a>), Dissect (<a href="">ICCV'25</a>), CateKV (<a href="https://openreview.net/pdf?id=5QWKec0eDF">ICML'25</a>)] </p>
                    </li>
                    <li>
                        <p>MedAI [medical pretraining & post-training: UniChest (<a href="https://arxiv.org/abs/2312.11038">TMI'24</a>), UniBrain (<a href="https://www.sciencedirect.com/science/article/abs/pii/S0895611125000254">CMIG'25</a>), AdaCoMed (<a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Multi-modal_Medical_Diagnosis_via_Large-small_Model_Collaboration_CVPR_2025_paper.pdf">CVPR'25</a>); adaptiation: LoRKD (<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhou_Low-Rank_Knowledge_Decomposition_for_Medical_Foundation_Models_CVPR_2024_paper.pdf">CVPR'24</a>), RD (<a href="https://arxiv.org/abs/2407.06504v1">MICCAI'24</a>)]</p>
                    </li>
                </ul>
            </div>

            <div>
                <h1>
                    <hr>
                    Generalized Imbalanced Learning
                </h1>
                <p>
                    Imbalanced Learning is an old topic in machine learning area, which is still lack of the solid foundation from theory to algorithm, although
                    it seems to be studied in (at least) two decades. The reason that we re-focus this problem is that the generalization of
                    algorithms in a broad sense has been recently drawn more attention, especially in the context of the pretraining paradigm. The underlying
                    evaluation metric on the holistic measure of each class, each task, each domain coincidentally is similar to the fine-grained measure in imbalanced
                    learning. This motivates us to extend imbalanced learning to boost these typical paradigms like self-supervised learning, weakly-supervised learning or generative modeling
                    to enhance the generalization. The following taxonomy is mainly based on the aspect that each research work considers, but actually these imbalance types may mix in practice.
                </p>
                <ul>
<!--                    <li>-->
<!--                        <p>Imbalanced learning with larger model capacity [<a href="https://dl.acm.org/doi/abs/10.1145/3447548.3467097">DCCL (KDD'21)</a>, <a href="https://dl.acm.org/doi/abs/10.1145/3534678.3539181">Meta Controller (KDD'22)</a>] </p>-->
<!--                    </li>-->
                    <li>
                        <p>Class imbalance [CBDM (<a href="https://openaccess.thecvf.com/content/CVPR2023/html/Qin_Class-Balancing_Diffusion_Models_CVPR_2023_paper.html">CVPR'23</a>), T2H (<a href="https://openreview.net/forum?id=NW2s5XXwXU">ICLR'24</a>), RDR (<a href="https://openreview.net/forum?id=vx4NgdyyVG">NeurIPS'24</a>)]</p>
                    </li>
                    <li>
                        <p>Spectral imbalance [FedGELA (<a href="https://openreview.net/forum?id=wwmKVO8bsR">NeurIPS'23</a>), DISAM (<a href="https://openreview.net/forum?id=I4wB3HA3dJ">ICLR'24</a>), MoLA (<a href="https://openreview.net/pdf?id=NQ6KDfSDFK">ICML'24</a>), PCD (<a href="https://openreview.net/pdf?id=AVrGtVrx10">NeurIPS'24</a>)]</p>
                    </li>
                    <li>
                        <p>Implicit imbalance [BCL (<a href="https://proceedings.mlr.press/v162/zhou22l.html">ICML'22</a>), GH (<a href="https://openreview.net/forum?id=geLARFEK8O">NeurIPS'23</a>), RECORDS (<a href="https://openreview.net/forum?id=sXfWoK4KvSW">ICLR'23</a>), SHE (<a href="https://openreview.net/forum?id=3GurO0kRue">ICLR'24</a>), BDR (<a href="https://arxiv.org/abs/2308.01698">TIP'24</a>), GeoCLIP (<a href="https://link.springer.com/article/10.1007/s10994-025-06745-w">MLJ'25</a>)]</p>
                    </li>
                </ul>
            </div>

            <div>
                <h1>
                    <hr>
                    Noise Robust Machine Learning
                </h1>
                <p>
                    Perturbation can be ubiquitous on real-world data and the proper mass can actually robustify the training of machine
                    learning models. This is common in the training practice like label smoothing, dropout and data augmentation with randomness. However,
                    when it is excessive or deliberate, or even only emerges during serving, the special design should be considered to reduce their negative
                    impact. Motivated by this belief, we developed a range of methods as references on this way.
                </p>
                <ul>
                    <li>
                        <p>Label-noise learning under the class-conditional assumption [<a href="https://dl.acm.org/doi/pdf/10.5555/3327345.3327485">NeurIPS'18</a>, <a href="https://ojs.aaai.org/index.php/AAAI/article/view/4943">AAAI'19</a>, <a href="https://www.computer.org/csdl/journal/tp/2023/08/10049697/1KYog7RZSUg">TPAMI'23</a>]</p>
                    </li>
                    <li>
                        <p>Label-noise learning with the selection, correction or regularization [<a href="http://proceedings.mlr.press/v97/yu19b/yu19b.pdf">ICML'19</a>, <a href="https://ieeexplore.ieee.org/abstract/document/8506425">TIP'19</a>, <a href="https://ojs.aaai.org/index.php/AAAI/article/view/17222">AAAI'21</a>, <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/435986a8cc3e0667648df5d1c2d55c83-Abstract-Conference.html">NeurIPS'23</a>, <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_Mitigating_Noisy_Correspondence_by_Geometrical_Structure_Consistency_Learning_CVPR_2024_paper.pdf">CVPR'24</a> <a href="https://arxiv.org/abs/2403.01942">ICML'24</a>, <a href="https://link.springer.com/article/10.1007/s10994-024-06678-w">MLJ'25</a>] </p>
                    </li>
                    <li>
                        <p>Adversarial learning with distillation, slack or representation learning [<a href="https://openreview.net/forum?id=u6TRGdzhfip">ICLR'22</a>, <a href="https://openreview.net/forum?id=eKllxpLOOm">ICLR'23</a>, <a href="https://openreview.net/pdf?id=Vcl3qckVyh">ICML'23</a>]</p>
                    </li>
                    <li>
                        <p>Out-of-Distribution Detection with intrinsic capacity, extrapolation or calibration [<a href="https://proceedings.mlr.press/v202/zhu23g/zhu23g.pdf">ICML'23</a>, <a href="https://openreview.net/pdf?id=RuxBLfiEqI">NeurIPS'23</a>, <a href="https://openreview.net/forum?id=w6vbfSC1y0">NeurIPS'24</a>] </p>
                    </li>
                </ul>
            </div>
        </td>

    </table>
</body>
</html>